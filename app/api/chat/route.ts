import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, AIMessage, SystemMessage, ToolMessage, BaseMessage, AIMessageChunk } from "@langchain/core/messages";
import { tool } from "@langchain/core/tools";
import { z } from "zod";
import { Sandbox } from "@e2b/code-interpreter";

export const runtime = "nodejs";
export const maxDuration = 60;

// Define tools the agent can use
const confirmAction = tool(
  async ({ action, description }) => {
    // This tool signals the frontend to show a confirmation dialog
    return JSON.stringify({
      type: "confirmation_required",
      action,
      description,
      status: "pending",
    });
  },
  {
    name: "confirm_action",
    description:
      "When you need user confirmation before performing a potentially dangerous or important action, use this tool. It will display a confirmation dialog to the user.",
    schema: z.object({
      action: z.string().describe("The action name that needs confirmation"),
      description: z
        .string()
        .describe("A detailed description of what will happen if confirmed"),
    }),
  }
);

const searchKnowledge = tool(
  async ({ query }) => {
    // Simulated knowledge search tool
    return JSON.stringify({
      type: "search_result",
      query,
      results: [
        `Found relevant information about: ${query}`,
        "This is a simulated search result for the MVP demo.",
      ],
    });
  },
  {
    name: "search_knowledge",
    description:
      "Search the knowledge base for relevant information. Use this when the user asks about specific topics.",
    schema: z.object({
      query: z.string().describe("The search query"),
    }),
  }
);

const calculateData = tool(
  async ({ expression }) => {
    try {
      // Simple safe eval for basic math (MVP demo)
      const result = Function(`"use strict"; return (${expression})`)();
      return JSON.stringify({ type: "calculation", expression, result });
    } catch {
      return JSON.stringify({
        type: "calculation_error",
        expression,
        error: "Invalid expression",
      });
    }
  },
  {
    name: "calculate",
    description: "Perform a mathematical calculation.",
    schema: z.object({
      expression: z.string().describe("The mathematical expression to evaluate"),
    }),
  }
);

const executePython = tool(
  async ({ code }) => {
    let sandbox: Sandbox | null = null;
    try {
      sandbox = await Sandbox.create({
        apiKey: process.env.E2B_API_KEY,
      });
      const execution = await sandbox.runCode(code);

      // Collect all output
      const stdout = execution.logs.stdout.join("\n");
      const stderr = execution.logs.stderr.join("\n");
      const results = execution.results.map((r) => ({
        text: r.text,
        png: r.png,  // base64 encoded image if any
        html: r.html,
      }));

      return JSON.stringify({
        type: "code_execution",
        code,
        stdout,
        stderr,
        results,
        error: execution.error ? execution.error.name + ": " + execution.error.value : null,
      });
    } catch (err: unknown) {
      const errMsg = err instanceof Error ? err.message : "Unknown error";
      return JSON.stringify({
        type: "code_execution_error",
        code,
        error: errMsg,
      });
    } finally {
      if (sandbox) {
        await sandbox.kill().catch(() => {});
      }
    }
  },
  {
    name: "execute_python",
    description:
      "Execute Python code in a fresh Jupyter notebook sandbox and return the result. IMPORTANT: Each invocation creates a NEW isolated sandbox â€” variables, imports, and files from previous executions are NOT available. Every code block MUST be fully self-contained with all imports, file reads (e.g. pd.read_csv('/home/user/xxx.csv')), and variable definitions. If a task has multiple steps, combine them into a single code block. The sandbox has common data science packages pre-installed (pandas, numpy, matplotlib, seaborn, scikit-learn, etc.).",
    schema: z.object({
      code: z
        .string()
        .describe(
          "The complete, self-contained Python code to execute. MUST include all necessary imports, data loading, and variable definitions â€” do NOT reference variables from previous executions."
        ),
    }),
  }
);

const tools = [confirmAction, searchKnowledge, calculateData, executePython];

function createModel() {
  return new ChatOpenAI({
    apiKey: process.env.OPENAI_API_KEY,
    configuration: {
      baseURL: process.env.OPENAI_BASE_URL,
    },
    model: "qwen3-coder",
    temperature: 0.7,
    streaming: true,
  }).bindTools(tools);
}

export async function POST(req: Request) {
  const { messages, files, toolResult, sessionFiles } = await req.json();

  const model = createModel();

  type ChatMsg = { role: string; content: string; tool_call_id?: string };
  interface FileInfo {
    name: string;
    size: number;
    preview: string;
    isGenerated?: boolean;
    richPreview?: {
      fileName: string;
      shape: [number, number];
      columns: string[];
      dtypes: Record<string, string>;
      head: string;
      describe: string;
      null_counts: Record<string, number>;
    };
  }

  // Helper: format a single file's context string
  function formatFileContext(f: FileInfo): string {
    const sourceTag = f.isGenerated ? " [ä»£ç æ‰§è¡Œç”Ÿæˆ]" : " [ç”¨æˆ·ä¸Šä¼ ]";
    if (f.richPreview) {
      const rp = f.richPreview;
      const nullInfo = Object.entries(rp.null_counts)
        .filter(([, v]) => v > 0)
        .map(([k, v]) => `  ${k}: ${v}ä¸ªç©ºå€¼`)
        .join("\n");
      return `ğŸ“ æ–‡ä»¶: ${f.name}${sourceTag} (${f.size} bytes)
ğŸ“Š æ•°æ®æ¦‚å†µ: ${rp.shape[0]}è¡Œ Ã— ${rp.shape[1]}åˆ—
ğŸ“‹ åˆ—å: ${rp.columns.join(", ")}
ğŸ“‹ åˆ—ç±»å‹:\n${Object.entries(rp.dtypes).map(([k, v]) => `  ${k}: ${v}`).join("\n")}
ğŸ“‹ å‰5è¡Œæ•°æ®:\n\`\`\`\n${rp.head}\n\`\`\`
ğŸ“‹ ç»Ÿè®¡æ‘˜è¦:\n\`\`\`\n${rp.describe}\n\`\`\`${nullInfo ? `\nâš ï¸ ç©ºå€¼æƒ…å†µ:\n${nullInfo}` : ""}
åœ¨Pythonä»£ç ä¸­ä½¿ç”¨è·¯å¾„: /home/user/${f.name}`;
    }
    return `ğŸ“ æ–‡ä»¶: ${f.name}${sourceTag} (${f.size} bytes)\næ–‡ä»¶å†…å®¹é¢„è§ˆ(å‰å‡ è¡Œ):\n\`\`\`\n${f.preview}\n\`\`\`\nåœ¨Pythonä»£ç ä¸­ä½¿ç”¨è·¯å¾„: /home/user/${f.name}`;
  }

  // Build session-level file context (injected into system prompt so it's always visible)
  let sessionFileContext = "";
  const effectiveSessionFiles: FileInfo[] = sessionFiles && Array.isArray(sessionFiles) && sessionFiles.length > 0
    ? sessionFiles
    : files && Array.isArray(files) && files.length > 0
      ? files
      : [];
  if (effectiveSessionFiles.length > 0) {
    sessionFileContext = "\n\nã€å½“å‰ä¼šè¯ä¸­å¯ç”¨çš„æ•°æ®æ–‡ä»¶ã€‘\n" +
      effectiveSessionFiles.map((f: FileInfo) => formatFileContext(f)).join("\n\n");
  }

  // Inject NEW file context into the last user message (so the LLM knows this turn has new files)
  let processedMessages: ChatMsg[] = messages;
  if (files && Array.isArray(files) && files.length > 0) {
    const newFileContext = files
      .map((f: FileInfo) => `\n\n` + formatFileContext(f))
      .join("");
    processedMessages = messages.map((msg: ChatMsg, index: number) => {
      if (index === messages.length - 1 && msg.role === "user") {
        return { ...msg, content: msg.content + newFileContext };
      }
      return msg;
    });
  }

  // Convert messages to LangChain format
  const langchainMessages = [
    new SystemMessage(
      `ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°æ®åˆ†æåŠ©æ‰‹ (Next Analyst)ã€‚ä½ å¯ä»¥å¸®åŠ©ç”¨æˆ·è¿›è¡Œæ•°æ®åˆ†æã€å›ç­”é—®é¢˜ã€æ‰§è¡Œè®¡ç®—ã€ç¼–å†™å’Œè¿è¡ŒPythonä»£ç ã€‚
      
ä½ æœ‰ä»¥ä¸‹å·¥å…·å¯ä»¥ä½¿ç”¨:
- execute_python: åœ¨Jupyteræ²™ç›’ä¸­æ‰§è¡ŒPythonä»£ç ï¼Œé€‚ç”¨äºæ•°æ®åˆ†æã€æ•°æ®å¤„ç†ã€ç”Ÿæˆå›¾è¡¨ã€å¤æ‚è®¡ç®—ç­‰åœºæ™¯ã€‚æ²™ç›’å·²é¢„è£… pandas, numpy, matplotlib, seaborn, scikit-learn ç­‰å¸¸ç”¨åº“ã€‚
- confirm_action: å½“éœ€è¦ç”¨æˆ·ç¡®è®¤é‡è¦æ“ä½œæ—¶ä½¿ç”¨
- search_knowledge: æœç´¢çŸ¥è¯†åº“è·å–ä¿¡æ¯
- calculate: æ‰§è¡Œç®€å•æ•°å­¦è®¡ç®—

é‡è¦è§„åˆ™:
1. å½“ç”¨æˆ·éœ€è¦æ•°æ®åˆ†æã€å¤„ç†æ•°æ®ã€ç”Ÿæˆå›¾è¡¨ã€æˆ–ä»»ä½•éœ€è¦ç¼–ç¨‹è§£å†³çš„é—®é¢˜æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨ execute_python å·¥å…·ã€‚
2. è¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚
3. å¦‚æœç”¨æˆ·è¦æ±‚æ‰§è¡Œé‡è¦æ“ä½œï¼ˆå¦‚åˆ é™¤æ•°æ®ã€ä¿®æ”¹é…ç½®ç­‰ï¼‰ï¼Œè¯·å…ˆä½¿ç”¨ confirm_action å·¥å…·è·å–ç”¨æˆ·ç¡®è®¤ã€‚
4. æ‰§è¡Œä»£ç åï¼Œè¯·è§£é‡Šä»£ç çš„æ‰§è¡Œç»“æœã€‚
5. å½“ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶æ—¶ï¼Œæ–‡ä»¶ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ°æ²™ç®±ç¯å¢ƒçš„ /home/user/ ç›®å½•ã€‚åœ¨Pythonä»£ç ä¸­é€šè¿‡è¯¥è·¯å¾„è¯»å–æ–‡ä»¶ï¼Œä¾‹å¦‚: pd.read_csv('/home/user/data.csv')ã€‚
6. ä½ ä¼šæ”¶åˆ°ç³»ç»Ÿé€šè¿‡æ²™ç›’è‡ªåŠ¨è§£æçš„æ•°æ®æ–‡ä»¶ç»“æ„ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼šåˆ—åã€æ•°æ®ç±»å‹ã€å‰5è¡Œæ•°æ®ã€ç»Ÿè®¡æ‘˜è¦å’Œç©ºå€¼æƒ…å†µã€‚è¯·å……åˆ†åˆ©ç”¨è¿™äº›ç»“æ„åŒ–ä¿¡æ¯æ¥åˆ¶å®šå‡†ç¡®çš„æ•°æ®åˆ†ææ–¹æ¡ˆï¼Œä¸è¦å‡è®¾æ–‡ä»¶æœ‰ä½ æœªçœ‹åˆ°çš„åˆ—æˆ–æ•°æ®ç»“æ„ã€‚
7. å¦‚æœæ”¶åˆ°æ–‡ä»¶çš„ç»“æ„åŒ–é¢„è§ˆä¿¡æ¯ï¼Œè¯·å…ˆå‘ç”¨æˆ·ç®€è¦æè¿°æ•°æ®æ¦‚å†µï¼ˆè¡Œåˆ—æ•°ã€ä¸»è¦å­—æ®µã€æ•°æ®ç±»å‹ç­‰ï¼‰ï¼Œç„¶åæ ¹æ®ç”¨æˆ·éœ€æ±‚åˆ¶å®šåˆ†ææ–¹æ¡ˆï¼Œæœ€åç”Ÿæˆæ‰§è¡Œä»£ç ã€‚
8. ã€ä»£ç å—ç‹¬ç«‹å®Œæ•´åŸåˆ™ã€‘æ¯æ¬¡è°ƒç”¨ execute_python æ—¶ï¼Œæ²™ç›’ç¯å¢ƒéƒ½æ˜¯å…¨æ–°çš„ï¼Œå‰ä¸€æ¬¡æ‰§è¡Œçš„å˜é‡ã€å¯¼å…¥çš„åº“ã€åŠ è½½çš„æ•°æ®åœ¨ä¸‹ä¸€æ¬¡æ‰§è¡Œä¸­ä¸å¯ç”¨ã€‚å› æ­¤ï¼Œæ¯ä¸ªä»£ç å—å¿…é¡»æ˜¯å®Œå…¨ç‹¬ç«‹çš„ã€å¯ç›´æ¥è¿è¡Œçš„å®Œæ•´ä»£ç ï¼ŒåŒ…æ‹¬:
   - æ‰€æœ‰å¿…è¦çš„ import è¯­å¥
   - é‡æ–°è¯»å–/åŠ è½½æ•°æ®æ–‡ä»¶ï¼ˆå¦‚ pd.read_csv('/home/user/xxx.csv')ï¼‰
   - é‡æ–°å®šä¹‰æ‰€æœ‰éœ€è¦ç”¨åˆ°çš„å˜é‡å’Œå‡½æ•°
   - ä¸å¾—å¼•ç”¨æˆ–ä¾èµ–å‰ä¸€ä¸ªä»£ç å—ä¸­å®šä¹‰çš„ä»»ä½•å˜é‡
   ç»å¯¹ä¸è¦å‡è®¾ä¹‹å‰çš„ä»£ç å—å·²ç»æ‰§è¡Œè¿‡æˆ–å…¶å˜é‡ä»ç„¶å­˜åœ¨ã€‚å¦‚æœä¸€ä¸ªåˆ†æä»»åŠ¡éœ€è¦å¤šä¸ªæ­¥éª¤ï¼Œå°½é‡å°†å®ƒä»¬åˆå¹¶åˆ°ä¸€ä¸ªä»£ç å—ä¸­æ‰§è¡Œã€‚
9. ã€ç”Ÿæˆæ–‡ä»¶è‡ªåŠ¨ä¿ç•™ã€‘ä»£ç æ‰§è¡Œä¸­ç”Ÿæˆçš„æ–°æ–‡ä»¶ï¼ˆå¦‚æ¸…æ´—åçš„æ•°æ®é›†ã€å¤„ç†ç»“æœç­‰ï¼‰ä¼šè‡ªåŠ¨ä¿ç•™åœ¨ä¼šè¯ä¸­ã€‚åç»­ä»£ç æ‰§è¡Œæ—¶ï¼Œè¿™äº›æ–‡ä»¶ä¼šè¢«è‡ªåŠ¨ä¸Šä¼ åˆ°æ²™ç›’çš„ /home/user/ ç›®å½•ï¼Œå¯ç›´æ¥é€šè¿‡è·¯å¾„è®¿é—®ã€‚å½“ä¼šè¯æ–‡ä»¶åˆ—è¡¨ä¸­åŒæ—¶å­˜åœ¨åŸå§‹æ–‡ä»¶å’Œå¤„ç†åçš„æ–‡ä»¶æ—¶ï¼Œè¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ–‡ä»¶è¿›è¡Œæ“ä½œï¼ˆé€šå¸¸åº”ä½¿ç”¨æœ€æ–°å¤„ç†è¿‡çš„æ–‡ä»¶ï¼‰ã€‚
10. ã€matplotlib å›¾è¡¨è§„èŒƒã€‘ç”Ÿæˆå›¾è¡¨æ—¶åŠ¡å¿…éµå®ˆä»¥ä¸‹è§„èŒƒä»¥é¿å…é‡å¤æ˜¾ç¤ºï¼š
   - è°ƒç”¨ plt.savefig() ä¿å­˜å›¾ç‰‡åï¼Œå¿…é¡»ç´§æ¥ç€è°ƒç”¨ plt.close('all') å…³é—­å›¾å½¢
   - ä¸è¦åœ¨ä¿å­˜åå†è°ƒç”¨ plt.show()
   - ä¸è¦ç”¨ PIL/Image æ‰“å¼€åˆšä¿å­˜çš„å›¾ç‰‡æ–‡ä»¶æ¥æ˜¾ç¤º
   - æ­£ç¡®æ¨¡å¼: plt.savefig('output.png', dpi=150, bbox_inches='tight'); plt.close('all'); print('å›¾è¡¨å·²ä¿å­˜ä¸º output.png')
   - å¦‚æœä¸éœ€è¦ä¿å­˜ä¸ºæ–‡ä»¶åªæ˜¯å±•ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ plt.show() ä½†ä¸è¦åŒæ—¶ savefig${sessionFileContext}`
    ),
    ...processedMessages.map(
      (msg: { role: string; content: string; tool_call_id?: string }) => {
        if (msg.role === "user") return new HumanMessage(msg.content);
        if (msg.role === "assistant") return new AIMessage(msg.content);
        return new HumanMessage(msg.content);
      }
    ),
  ];

  // If this is a continuation after HITL tool execution, add the result context
  if (toolResult) {
    const resultParts: string[] = [];
    if (toolResult.code) resultParts.push(`æ‰§è¡Œçš„ä»£ç :\n\`\`\`python\n${toolResult.code}\n\`\`\``);
    if (toolResult.stdout) resultParts.push(`æ ‡å‡†è¾“å‡º:\n${toolResult.stdout}`);
    if (toolResult.stderr) resultParts.push(`æ ‡å‡†é”™è¯¯:\n${toolResult.stderr}`);
    if (toolResult.error) resultParts.push(`é”™è¯¯: ${toolResult.error}`);
    if (toolResult.results && toolResult.results.length > 0) {
      const descriptions = toolResult.results
        .map((r: { text?: string; png?: string; html?: string }) => {
          if (r.png) return "[ç”Ÿæˆäº†å›¾è¡¨]";
          if (r.text) return r.text;
          if (r.html) return "[ç”Ÿæˆäº†HTMLå†…å®¹]";
          return "";
        })
        .filter(Boolean);
      if (descriptions.length > 0) resultParts.push(`æ‰§è¡Œç»“æœ:\n${descriptions.join("\n")}`);
    }
    if (toolResult.generatedFiles && Array.isArray(toolResult.generatedFiles) && toolResult.generatedFiles.length > 0) {
      const fileDescs = toolResult.generatedFiles
        .map((f: { name: string; size: number }) => `- /home/user/${f.name} (${f.size} bytes)`)
        .join("\n");
      resultParts.push(`ç”Ÿæˆçš„æ–°æ–‡ä»¶:\n${fileDescs}\nè¿™äº›æ–‡ä»¶å·²ä¿å­˜åœ¨ä¼šè¯ä¸­ï¼Œåç»­ä»£ç æ‰§è¡Œæ—¶å¯é€šè¿‡ä¸Šè¿°è·¯å¾„è®¿é—®ã€‚`);
    }
    langchainMessages.push(
      new HumanMessage(
        `[ç³»ç»Ÿæ¶ˆæ¯] ä¹‹å‰è¯·æ±‚çš„Pythonä»£ç å·²æ‰§è¡Œå®Œæ¯•ï¼Œç»“æœå¦‚ä¸‹:\n\n${resultParts.join("\n\n")}\n\nè¯·æ ¹æ®æ‰§è¡Œç»“æœä¸ºç”¨æˆ·æä¾›åˆ†æå’Œè§£é‡Šã€‚`
      )
    );
  }

  // Create a streaming response with agent loop
  const encoder = new TextEncoder();
  const stream = new ReadableStream({
    async start(controller) {
      try {
        let currentMessages: BaseMessage[] = [...langchainMessages];
        const MAX_ITERATIONS = 10;

        for (let iter = 0; iter < MAX_ITERATIONS; iter++) {
          let response = new AIMessageChunk({ content: "" });
          const stream = await model.stream(currentMessages);

          for await (const chunk of stream) {
            response = response.concat(chunk);
            if (chunk.content && typeof chunk.content === "string") {
              const content = chunk.content;
              if (content) {
                controller.enqueue(
                  encoder.encode(
                    `data: ${JSON.stringify({
                      type: "text_delta",
                      content: content,
                    })}\n\n`
                  )
                );
              }
            }
          }

          // Check if there are tool calls
          if (response.tool_calls && response.tool_calls.length > 0) {
            // Text content was already streamed via text_delta

            let hasPendingToolCall = false;
            const toolMessages: ToolMessage[] = [];

            for (const toolCall of response.tool_calls) {
              const matchedTool = tools.find((t) => t.name === toolCall.name);
              if (!matchedTool) continue;

              if (toolCall.name === "execute_python") {
                hasPendingToolCall = true;
                controller.enqueue(
                  encoder.encode(
                    `data: ${JSON.stringify({
                      type: "pending_tool_call",
                      tool: toolCall.name,
                      args: toolCall.args,
                    })}\n\n`
                  )
                );
              } else {
                // eslint-disable-next-line @typescript-eslint/no-explicit-any
                const toolExecResult = await (matchedTool as any).invoke(
                  toolCall.args
                );
                controller.enqueue(
                  encoder.encode(
                    `data: ${JSON.stringify({
                      type: "tool_call",
                      tool: toolCall.name,
                      args: toolCall.args,
                      result: JSON.parse(toolExecResult as string),
                    })}\n\n`
                  )
                );
                toolMessages.push(
                  new ToolMessage({
                    tool_call_id: toolCall.id!,
                    content: toolExecResult as string,
                  })
                );
              }
            }

            if (hasPendingToolCall) {
              // Can't continue loop - need user approval first
              break;
            }

            // All tools executed, feed results back to model for follow-up
            // We use the full accumulated response as the AI message
            currentMessages = [...currentMessages, response, ...toolMessages];
            continue;
          }

          // No tool calls - streaming already finished
          break;
        }

        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify({ type: "done" })}\n\n`)
        );
        controller.close();
      } catch (error: unknown) {
        const errorMessage =
          error instanceof Error ? error.message : "Unknown error";
        controller.enqueue(
          encoder.encode(
            `data: ${JSON.stringify({
              type: "error",
              content: `Error: ${errorMessage}`,
            })}\n\n`
          )
        );
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}
